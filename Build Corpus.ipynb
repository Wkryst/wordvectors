{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import codecs\n",
    "import lxml.etree as etree\n",
    "import os\n",
    "import regex\n",
    "import sys\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir, getsize\n",
    "\n",
    "lcode = 'pl'\n",
    "max_corpus_size = 100000000000\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\n",
    "# archive_date='20170820'\n",
    "# arch_uri=\"https://dumps.wikimedia.org/plwiki/20170820/\"\n",
    "# file = \"{}wiki-{}-pages-articles-multistream.xml\".format(lcode,archive_date)\n",
    "\n",
    "if lcode == 'ko':\n",
    "    from konlpy.tag import Kkma # pip install konlpy. See http://konlpy.org/en/v0.4.4/ for further information.\n",
    "    kkma = Kkma()\n",
    "    print (\"kkma succesfuly loaded!\")\n",
    "elif lcode == 'ja':\n",
    "    import MeCab # See https://pypi.python.org/pypi/mecab-python/0.996\n",
    "    mecab = MeCab.Tagger(\"-Owakati\")\n",
    "    print (\"mecab succesfuly loaded!\")\n",
    "elif lcode == 'zh':\n",
    "    import jieba # See https://pypi.python.org/pypi/jieba/\n",
    "    print (\"jieba succesfuly loaded!\")\n",
    "elif lcode == 'vi':\n",
    "    from pyvi.pyvi import ViTokenizer # See https://pypi.python.org/pypi/pyvi\n",
    "    print (\"pyvi succesfuly loaded!\")\n",
    "elif lcode == 'th':  \n",
    "    import pythai # See https://pypi.python.org/pypi/pythai  \n",
    "    print (\"pythai succesfuly loaded!\")\n",
    "\n",
    "    \n",
    "#https://dumps.wikimedia.org/plwiki/20170820/\n",
    "#wget \"https://dumps.wikimedia.org/${lcode}wiki/20170820/${lcode}wiki-20170820-pages-articles-multistream.xml.bz2\"\n",
    "#http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/\n",
    "\n",
    "def download_dump(arch_uri=\"https://dumps.wikimedia.org/plwiki/20170820/\", \n",
    "                  file=\"plwiki-20170820-pages-articles-multistream.xml.bz2\"):\n",
    "    datafile=\"data/{}\".format(file)\n",
    "    if not (isfile(datafile) or isfile(datafile[:-4])):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc=file) as pbar:\n",
    "            urlretrieve(arch_uri+file, datafile, pbar.hook)\n",
    "    print (\"Downloading DONE\")\n",
    " \n",
    "    return datafile\n",
    "\n",
    "# XXX: Python sucsk at extracting files like hell...\n",
    "# Use plain bzip2/gzip/tar in %%bash\n",
    "# def unbzip(filepath):\n",
    "#     import bz2\n",
    "#     newfilepath = filepath[:-4]\n",
    "#     fsize=getsize(filepath)\n",
    "#     block = 100*1024 # 100 * 1024 if fsize//1024 >=100 else fsize//2\n",
    "#     print (\"Unpacking {} \".format(filepath))\n",
    "#     with open(newfilepath, 'wb') as new_file, bz2.BZ2File(filepath, 'rb') as file:\n",
    "#         for data in (iter(lambda : file.read(block), 'rb')):\n",
    "#             new_file.write(data)\n",
    "            \n",
    "# #     with open(newfilepath, 'wb') as new_file, open(filepath, 'rb') as file:\n",
    "# #         decompressor = bz2.BZ2Decompressor()\n",
    "# #         for data in tqdm(iter(lambda : file.read(block), b'')):\n",
    "# #             new_file.write(decompressor.decompress(data))\n",
    "#     return newfilepath\n",
    "\n",
    "# def extract(filepath):\n",
    "#     import tarfile\n",
    "#     tar = tarfile.open(filepath, \"r:bz2\")\n",
    "#     tar.extractall()\n",
    "#     tar.close()\n",
    "\n",
    "def unbzip2(filepath):\n",
    "    bashCommand = [\"bzip2\",'-d', filepath]\n",
    "    try:\n",
    "        output = subprocess.check_output(bashCommand, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as pserror: \n",
    "        print (pserror.output)\n",
    "    else:\n",
    "        print (\"DONE {}\".format(output))\n",
    "    return filepath[:-4]\n",
    "\n",
    "def extract(filepath):\n",
    "    bashCommand = [\"tar\",'-xvf', filepath, '-C','data']\n",
    "    try:\n",
    "        output = subprocess.check_output(bashCommand, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as pserror: \n",
    "        print (pserror.output)\n",
    "    else:\n",
    "        print (\"DONE {}\".format(output))\n",
    "    return filepath[:-3]\n",
    "\n",
    "\n",
    "def clean_text(text, lcode):\n",
    "    \n",
    "    # Common\n",
    "    text = regex.sub(\"(?s)<ref>.+?</ref>\", \"\", text) # remove reference links\n",
    "    text = regex.sub(\"(?s)<[^>]+>\", \"\", text) # remove html tags\n",
    "    text = regex.sub(\"&[a-z]+;\", \"\", text) # remove html entities\n",
    "    text = regex.sub(\"(?s){{.+?}}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s){.+?}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\|)\", \"\", text) # remove link target strings\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\:.+?]])\", \"\", text) # remove media links\n",
    "    \n",
    "    text = regex.sub(\"[']{5}\", \"\", text) # remove italic+bold symbols\n",
    "    text = regex.sub(\"[']{3}\", \"\", text) # remove bold symbols\n",
    "    text = regex.sub(\"[']{2}\", \"\", text) # remove italic symbols\n",
    "    \n",
    "    if lcode in ['ko']: # korean\n",
    "        text = regex.sub(u\"[^ \\r\\n\\p{Hangul}.?!]\", \" \", text) # Replace unacceptable characters with a space.\n",
    "    elif lcode in ['ja']: # japanese\n",
    "        text = regex.sub(u\"[^\\r\\n\\p{Han}\\p{Hiragana}\\p{Katakana}ー。！？]\", \"\", text)\n",
    "    elif lcode in ['zh']: # chinsese\n",
    "        text = regex.sub(u\"[^\\r\\n\\p{Han}。！？]\", \"\", text)\n",
    "    elif lcode in ['th']: # thai\n",
    "        text = regex.sub(u\"[^ \\r\\n\\p{Thai}.?!]\", \" \", text)\n",
    "    elif lcode in ['ru']: # russian\n",
    "        text = regex.sub(u\"[^ \\r\\n\\p{Cyrillic}.?!\\-]\", \" \", text)\n",
    "        text = text.lower()\n",
    "#     elif lcode in ['ar']: # arabic\n",
    "#         text = regex.sub(u\"[^ \\r\\n\\p{Arabic}.?!\\-]\", \" \", text)\n",
    "    elif lcode in ['hi']: # hindi\n",
    "        text = regex.sub(u\"[^ \\r\\n\\p{Devanagari}.।?!\\-]\", \" \", text)\n",
    "    elif lcode in ['bn']: # bengali\n",
    "        text = regex.sub(u\"[^ \\r\\n\\p{Bengali}.।?!\\-]\", \" \", text)\n",
    "    elif lcode in ['de']: # german\n",
    "        text = regex.sub(u\"[^ \\r\\n\\p{Latin}\\-'‘’.?!]\", \" \", text)\n",
    "    else: # Mostly european languages\n",
    "        text = regex.sub(u\"[^ \\r\\n\\p{Latin}\\-'‘’.?!]\", \" \", text)\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Common\n",
    "    text = regex.sub(\"[ ]{2,}\", \" \", text) # Squeeze spaces.\n",
    "    return text\n",
    "\n",
    "def sentence_segment(text, lcode):\n",
    "    '''\n",
    "    Args:\n",
    "      text: A string. A unsegmented paragraph.\n",
    "    \n",
    "    Returns:\n",
    "      A list of sentences.\n",
    "    '''\n",
    "    if lcode in ['ja', 'zh']:\n",
    "        sents = regex.split(u\"([。！？])?[\\n]+|[。！？]\", text) \n",
    "    elif lcode in ['th']:\n",
    "        sents = text.split(\"[\\n]+\") \n",
    "    elif lcode in ['hi', 'bn']: # hindi, bengali\n",
    "        sents = regex.split(u\"([.।?!])?[\\n]+|[.।?!] \", text)\n",
    "    elif lcode in ['de']: # german\n",
    "        sents = regex.split(\"([.?!])?[\\n]+|[.?!] \", text)\n",
    "        sents = [sent[0].lower() + sent[1:] for sent in sents if sent is not None and len(sent) > 1]\n",
    "    else:\n",
    "        sents = regex.split(\"([.?!])?[\\n]+|[.?!] \", text)\n",
    "    return sents\n",
    "        \n",
    "def word_segment(sent, lcode):\n",
    "    '''\n",
    "    Args:\n",
    "      sent: A string. A sentence.\n",
    "    \n",
    "    Returns:\n",
    "      A list of words.\n",
    "    '''\n",
    "    if lcode in ['ko']:\n",
    "        words = [word for word, _ in kkma.pos(sent)]\n",
    "    elif lcode in ['ja']:\n",
    "        words = mecab.parse(sent.encode('utf8')).split() \n",
    "    elif lcode in ['th']:\n",
    "        words = pythai.split(sent)\n",
    "    elif lcode in ['vi']:\n",
    "        words = ViTokenizer.tokenize(sent).split()        \n",
    "    elif lcode in ['zh']:\n",
    "        words = list(jieba.cut(sent, cut_all=False)) \n",
    "#     elif lcode in ['ar']:\n",
    "#         words = segmenter.segment(sent).split()\n",
    "    else: # Mostly european languages\n",
    "        words = sent.split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "def build_corpus(filepath, max_corpus_size=100000000000, lcode=\"pl\"):\n",
    "    txt_file=datafile=\"{}.txt\".format(filepath[:-4])\n",
    "    if isfile(txt_file):\n",
    "        print (\"Corpus file {} exists.\".format(txt_file))\n",
    "        return\n",
    "    with codecs.open(txt_file, 'w', 'utf-8') as fout:\n",
    "        i = 1\n",
    "        j = 1\n",
    "        ns = \"{http://www.mediawiki.org/xml/export-0.10/}\" # namespace\n",
    "        for _, elem in tqdm(etree.iterparse(filepath, tag=ns+\"text\")):\n",
    "            running_text = elem.text\n",
    "            try:\n",
    "                running_text = clean_text(running_text, lcode)\n",
    "                sents = sentence_segment(running_text, lcode)\n",
    "                for sent in sents:\n",
    "                    if sent is not None:\n",
    "                        words = word_segment(sent, lcode)\n",
    "                        if len(words) > 10:\n",
    "                            if lcode in ['ja']:\n",
    "                                fout.write(\" \".join(words).decode('utf8') + \"\\n\")\n",
    "                            else:\n",
    "                                fout.write(\" \".join(words) + \"\\n\")\n",
    "                                \n",
    "            except:\n",
    "                continue # it's okay as we have a pretty big corpus!\n",
    "            elem.clear() # We need to save memory!\n",
    "            if i % 1000 == 0: \n",
    "                #print ('.', end='')\n",
    "                fsize = os.path.getsize(txt_file)\n",
    "                if fsize > max_corpus_size:\n",
    "                    break\n",
    "            i += 1\n",
    "    print (\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File present: data/plwiki-20170820-pages-articles-multistream.xml\n",
      "Corpus file data/plwiki-20170820-pages-articles-multistream.txt exists.\n"
     ]
    }
   ],
   "source": [
    "wiki=download_dump()\n",
    "build_corpus(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading DONE\n",
      "b\"bzip2: Can't open input file data/plwikibooks-20170820-pages-articles-multistream.xml.bz2: No such file or directory.\\n\"\n",
      "Corpus file data/plwikibooks-20170820-pages-articles-multistream.txt exists.\n"
     ]
    }
   ],
   "source": [
    "wikibooks=download_dump(arch_uri=\"https://dumps.wikimedia.org/plwikibooks/20170820/\" ,file=\"plwikibooks-20170820-pages-articles-multistream.xml.bz2\")\n",
    "wikibooks=unbzip2(wikibooks)\n",
    "build_corpus(wikibooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File present: data/plwiktionary-20170820-pages-articles-multistream.xml\n",
      "Corpus file data/plwiktionary-20170820-pages-articles-multistream.txt exists.\n"
     ]
    }
   ],
   "source": [
    "wiktionary=download_dump(arch_uri=\"https://dumps.wikimedia.org/plwiktionary/20170820/\" ,file=\"plwiktionary-20170820-pages-articles-multistream.xml.bz2\")\n",
    "build_corpus(wiktionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSubtitles corpus \n",
    "The folowing code was used to extract OpenSubtitles corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/pl.ta'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitles=download_dump(arch_uri=\"http://opus.lingfil.uu.se/download.php?f=OpenSubtitles2016/\", file=\"pl.tar.gz\")\n",
    "extract(subtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated data/OpenSubtitles2016.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "truncatefile > data/OpenSubtitles2016.txt 2&> /dev/null\n",
    "echo \"Truncated data/OpenSubtitles2016.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 ms, sys: 0 ns, total: 40 ms\n",
      "Wall time: 40.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from io import StringIO, BytesIO\n",
    "def build_OpenSubtitlesCorpus(filepath, max_corpus_size=100000000000, lcode=\"pl\", window_size=5, txt_file=\"data/OpenSubtitles2016.txt\"):\n",
    "    with codecs.open(txt_file, 'a', 'utf-8') as fout:\n",
    "        i = 1\n",
    "        j = 1\n",
    "  \n",
    "        tree= etree.parse(filepath)\n",
    "        s= ((etree.tostring(tree.getroot()) ))\n",
    "        s=BytesIO(s)\n",
    "\n",
    "        doc=[]\n",
    "        sent=[]\n",
    "        for action, elem in etree.iterparse(s, tag=['w','s']):\n",
    "            \n",
    "            # append all words in sentence\n",
    "            if elem.text is not None and elem.text!=\"\":\n",
    "                sent.append(elem.text)\n",
    "                \n",
    "            # when the sentence is finished\n",
    "            if (action=='end' and elem.tag=='s'):\n",
    "                #print(sent)\n",
    "                if sent is not None:\n",
    "                    doc.append(\" \".join(sent))\n",
    "                sent=[]\n",
    "\n",
    "        doc=\"\".join(doc)\n",
    "        doc = regex.sub(u\"[^ \\n\\p{Latin}\\-'‘’.?!]\", \" \", doc) # clean text\n",
    "        doc = regex.sub(u\" - \", \" \", doc) # Squeeze spaces\n",
    "        doc = regex.sub(u\"\\n *\", \"\\n\", doc) # Squeeze spaces\n",
    "        doc = regex.sub(u\" \\n\", \"\\n\", doc) # Squeeze spaces\n",
    "        doc = regex.sub(u\" +\", \" \", doc) # Squeeze spaces        \n",
    "        #doc = regex.sub(\"\\n | \\n|^\\- \", \"\", doc) # leading/trailing spaces and dialogs\n",
    "\n",
    "        fout.write(doc.lower()+\"\\n\")\n",
    "\n",
    "                \n",
    "#build_OpenSubtitlesCorpus(\"data/OpenSubtitles2016/xml/pl/1998/568102/3380400.xml.gz\", txt_file=\"OS-test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "CPU times: user 1h 11min 45s, sys: 20.4 s, total: 1h 12min 6s\n",
      "Wall time: 1h 13min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for root, subdirs, files in (os.walk(\"data/OpenSubtitles2016/xml/pl\")):\n",
    "    for filename in (files):\n",
    "        file_path = os.path.join(root, filename)\n",
    "        build_OpenSubtitlesCorpus(file_path)\n",
    "        #print('\\t- file %s (full path: %s)' % (filename, file_path))\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
