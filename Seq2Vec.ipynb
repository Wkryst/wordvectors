{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load word2vec model\n",
    "w2v = gensim.models.KeyedVectors.load('data/w2v-773752559-1000000-300-5-5-OpenSubtitles2016.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(output_dim=300):\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning_rate and input_sequence_length.\n",
    "    :return: Tuple (input_, targets, learning_rate, keep_prob, input_sequence_length)\n",
    "    \"\"\"\n",
    "\n",
    "    input_ = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.float32, [None, output_dim])\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    input_sequence_length = tf.placeholder(tf.int32, [None], name=\"input_sequence_length\")\n",
    "    \n",
    "    return (input_, targets, learning_rate, keep_prob, input_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob, inputs, num_classes, input_sequence_length):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # one_hot encode input\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes) # num_classes = len(vocab)\n",
    "    \n",
    "    def build_cell(rnn_size):\n",
    "        cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        return cell\n",
    "    \n",
    "    # Construct a stacked tf.contrib.rnn.LSTMCell...\n",
    "    stacked_cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size) for _ in range(num_layers)])\n",
    "    # ...wrapped in a tf.contrib.rnn.DropoutWrapper\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(stacked_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Pass cell and embedded input to tf.nn.dynamic_rnn()\n",
    "    rnn_output, rnn_state = tf.nn.dynamic_rnn(cell, x_one_hot, sequence_length=input_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    # Initial state\n",
    "    initial_state = tf.identity(stacked_cell.zero_state(batch_size, tf.float32), name=\"initial_state\")\n",
    "    \n",
    "    return rnn_output, rnn_state, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about the final rnn cell output. So we need to grab it with outputs[:, -1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(cell, output_dim):\n",
    "    return tf.contrib.layers.fully_connected(inputs=cell[:, -1], num_outputs=output_dim, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(pred, Y):\n",
    "    pred=tf.nn.l2_normalize(pred,0)\n",
    "    Y=tf.nn.l2_normalize(Y,0)\n",
    "    \n",
    "    return tf.losses.cosine_distance(pred, Y, dim=1)\n",
    "    #return tf.reduce_sum(tf.multiply(pred,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "vocab = sorted(set(\" \".join(w2v.wv.index2word)))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab,1 )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "lstm_size=512\n",
    "num_layers=2\n",
    "keep_probability=0.8\n",
    "num_classes=len(vocab)\n",
    "output_dim=300\n",
    "num_epochs=100\n",
    "learning_rate=0.001\n",
    "save_dir = './save'\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    (input_, targets, lr, keep_prob, input_sequence_length) = get_inputs()\n",
    "    rnn_output, rnn_state, initial_state = build_lstm(lstm_size, num_layers, batch_size, keep_prob, input_, num_classes, input_sequence_length)\n",
    "    output = build_output(rnn_output, output_dim)\n",
    "    loss = get_loss(output, targets)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    # clip gradients\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    \n",
    "        \n",
    "#     # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "#     tvars = tf.trainable_variables()\n",
    "#     grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "#     train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "#     optimizer = train_op.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def word2seq(word):\n",
    "#     return np.array([vocab_to_int.get(c,0) for c in word])\n",
    "\n",
    "# def get_train_subset(model=w2v, seed_words=500, topn=7):\n",
    "#     top_words=model.wv.index2word[120:120+seed_words]\n",
    "#     top_words=np.append(np.array(top_words),np.array([np.array(model.most_similar_cosmul(w, topn=topn))[:,0] for w in top_words]))\n",
    "#     top_words=top_words.flatten()\n",
    "#     top_words=set(top_words)\n",
    "#     return top_words\n",
    "\n",
    "# input_list=list(get_train_subset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_2_int(train_set=train_set):\n",
    "#     return [[vocab_to_int[w] for w in l] for l in train_set]\n",
    "#     pass\n",
    "\n",
    "\n",
    "def get_padded_int_batch(input_batch, vocab_to_int):\n",
    "    max_len = max([len(word) for word in input_batch])\n",
    "    int_batch =  [[0] * (max_len - len(l)) + [vocab_to_int[w] for w in l] for l in input_batch]\n",
    "    return int_batch\n",
    "\n",
    "\n",
    "def get_batch(input_list=w2v.wv.index2word, batch_size=batch_size, vocab=vocab, vocab_to_int=vocab_to_int, model=w2v):\n",
    "    \"\"\"\n",
    "    Batch generator.\n",
    "    Input: train_set - list of words\n",
    "    Returns touple:\n",
    "    (pad_input_batch, pad_input_lengths, output_batch)\n",
    "    \"\"\"\n",
    "    for batch_i in range(0, len(input_list)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        input_batch = input_list[start_i:start_i + batch_size]\n",
    "#         print(input_batch)\n",
    "\n",
    "        # Pad\n",
    "        pad_input_batch = np.array(get_padded_int_batch(input_batch, vocab_to_int))\n",
    "#         print (pad_input_batch)\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_input_lengths = []\n",
    "        for line in pad_input_batch:\n",
    "            pad_input_lengths.append(len(line))\n",
    "\n",
    "            \n",
    "        # output batch\n",
    "        output_batch=np.array([w2v.wv.word_vec(w) for w in input_batch])\n",
    "        # print (output_batch)\n",
    "        # print (pad_input_lengths)\n",
    "        yield (pad_input_batch, pad_input_lengths, output_batch)\n",
    "\n",
    "\n",
    "# for (batch_i, (pad_input_batch, pad_input_lengths, output)) in enumerate(get_batch(w2v.wv.index2word[:1000], batch_size=50)):\n",
    "#     print (batch_i)\n",
    "#     pass\n",
    "\n",
    "train_input = w2v.wv.index2word[:10000]\n",
    "valid_input = w2v.wv.index2word[10000:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.687967\n",
      "0.68533\n",
      "0.679318\n",
      "0.672336\n",
      "0.668946\n",
      "0.657595\n",
      "0.636338\n",
      "0.614889\n",
      "0.598381\n",
      "0.579278\n",
      "0.568593\n",
      "0.555491\n",
      "0.545966\n",
      "0.533061\n",
      "0.521892\n",
      "0.510227\n",
      "0.497467\n",
      "0.488603\n",
      "0.476933\n",
      "0.460777\n",
      "0.447078\n",
      "0.432701\n",
      "0.420488\n",
      "0.411255\n",
      "0.399369\n",
      "0.388296\n",
      "0.371705\n",
      "0.363467\n",
      "0.349629\n",
      "0.344336\n",
      "0.335385\n",
      "0.326176\n",
      "0.313569\n",
      "0.303563\n",
      "0.29783\n",
      "0.290841\n",
      "0.279991\n",
      "0.277437\n",
      "0.278125\n",
      "0.266666\n",
      "0.261025\n",
      "0.255475\n",
      "0.253872\n",
      "0.245984\n",
      "0.244423\n",
      "0.23966\n",
      "0.233882\n",
      "0.228223\n",
      "0.221512\n",
      "0.218173\n",
      "0.217728\n",
      "0.21241\n",
      "0.207996\n",
      "0.204128\n",
      "0.200776\n",
      "0.198734\n",
      "0.192925\n",
      "0.190086\n",
      "0.184505\n",
      "0.179385\n",
      "0.17884\n",
      "0.175199\n",
      "0.17434\n",
      "0.173626\n",
      "0.171723\n",
      "0.168765\n",
      "0.164635\n",
      "0.159555\n",
      "0.156445\n",
      "0.159174\n",
      "0.15509\n",
      "0.150107\n",
      "0.150091\n",
      "0.1499\n",
      "0.144651\n",
      "0.144699\n",
      "0.142424\n",
      "0.139775\n",
      "0.136731\n",
      "0.135755\n",
      "0.134516\n",
      "0.130349\n",
      "0.131222\n",
      "0.127474\n",
      "0.124283\n",
      "0.124255\n",
      "0.120589\n",
      "0.119806\n",
      "0.1194\n",
      "0.115225\n",
      "0.117116\n",
      "0.114234\n",
      "0.113239\n",
      "0.111028\n",
      "0.109572\n",
      "0.105979\n",
      "0.104174\n",
      "0.105864\n",
      "0.102838\n",
      "0.103081\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        for batch_i, (pad_input_batch, pad_input_lengths, out_vec) in enumerate(get_batch(train_input)):\n",
    "            # reset state\n",
    "            state = sess.run(initial_state, {\n",
    "                input_: pad_input_batch,\n",
    "                input_sequence_length: pad_input_lengths\n",
    "                                            })\n",
    "            _, l = sess.run([train_op, loss], {\n",
    "                input_: pad_input_batch,\n",
    "                targets: out_vec,\n",
    "                lr: learning_rate, \n",
    "                keep_prob: keep_probability,\n",
    "                input_sequence_length: pad_input_lengths,  \n",
    "            })\n",
    "        print(l)\n",
    "        \n",
    "\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    (pad_input_batch, pad_input_lengths, out_vec) = next(get_batch(valid_input))\n",
    "    state = sess.run(initial_state, {\n",
    "        input_: pad_input_batch,\n",
    "        input_sequence_length: pad_input_lengths\n",
    "        })\n",
    "    outputs = sess.run(output,{\n",
    "        input_: pad_input_batch,\n",
    "        keep_prob: 1.0,\n",
    "        input_sequence_length: pad_input_lengths,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namówić\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sangrita', 0.18988117575645447),\n",
       " ('łuke', 0.18721675872802734),\n",
       " ('qingkong', 0.18496420979499817),\n",
       " ('kosmicznykowboj', 0.18381938338279724),\n",
       " ('zioberek', 0.1813872754573822),\n",
       " ('śnieżek', 0.1782398372888565),\n",
       " ('naprałdę', 0.17739275097846985),\n",
       " ('pielucho-majtki', 0.17712533473968506),\n",
       " ('a.d.d.a.d.d.a.d.d.a.d.d.', 0.17548315227031708),\n",
       " ('fioletowego', 0.17520149052143097)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs[0]\n",
    "\n",
    "#outputs[0] - w2v.wv.word_vec(valid_input[0])\n",
    "print(valid_input[0])\n",
    "w2v.wv.similar_by_vector(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
