{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load word2vec model\n",
    "w2v = gensim.models.KeyedVectors.load('data/w2v-773752559-1000000-300-5-5-OpenSubtitles2016.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(output_dim=300):\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning_rate and input_sequence_length.\n",
    "    :return: Tuple (input_, targets, learning_rate, keep_prob, input_sequence_length)\n",
    "    \"\"\"\n",
    "\n",
    "    input_ = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.float32, [None, output_dim])\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    input_sequence_length = tf.placeholder(tf.int32, [None], name=\"input_sequence_length\")\n",
    "    \n",
    "    return (input_, targets, learning_rate, keep_prob, input_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob, inputs, num_classes, input_sequence_length):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # one_hot encode input\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes) # num_classes = len(vocab)\n",
    "    \n",
    "    def build_cell(rnn_size):\n",
    "        cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        return cell\n",
    "    \n",
    "    # Construct a stacked tf.contrib.rnn.LSTMCell...\n",
    "    stacked_cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size) for _ in range(num_layers)])\n",
    "    # ...wrapped in a tf.contrib.rnn.DropoutWrapper\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(stacked_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Pass cell and embedded input to tf.nn.dynamic_rnn()\n",
    "    rnn_output, rnn_state = tf.nn.dynamic_rnn(cell, x_one_hot, sequence_length=input_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    # Initial state\n",
    "    initial_state = tf.identity(stacked_cell.zero_state(batch_size, tf.float32), name=\"initial_state\")\n",
    "    \n",
    "    return rnn_output, rnn_state, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about the final rnn cell output. So we need to grab it with outputs[:, -1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(cell, keep_prob, output_dim=300):\n",
    "    dense = tf.contrib.layers.fully_connected(inputs=cell[:, -1], num_outputs=output_dim*2, activation_fn=tf.nn.relu)\n",
    "    dense = tf.nn.dropout(dense, keep_prob)\n",
    "    return tf.contrib.layers.fully_connected(dense, num_outputs=output_dim, activation_fn=None)\n",
    "#     return tf.contrib.layers.fully_connected(inputs=cell[:, -1], num_outputs=output_dim, activation_fn=tf.nn.relu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(pred, Y):\n",
    "    pred=tf.nn.l2_normalize(pred,0)\n",
    "    Y=tf.nn.l2_normalize(Y,0)\n",
    "    return tf.reduce_sum(tf.multiply(pred,Y))\n",
    "#     return tf.losses.cosine_distance(pred, Y, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "vocab = sorted(set(\" \".join(w2v.wv.index2word)))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab,1 )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "lstm_size=1024\n",
    "num_layers=3\n",
    "keep_probability=1.0#0.8\n",
    "num_classes=len(vocab)\n",
    "output_dim=300\n",
    "learning_rate=0.001\n",
    "save_dir = './seq2vec'\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    (input_, targets, lr, keep_prob, input_sequence_length) = get_inputs()\n",
    "    rnn_output, rnn_state, initial_state = build_lstm(lstm_size, num_layers, batch_size, keep_prob, input_, num_classes, input_sequence_length)\n",
    "    output = build_output(rnn_output, keep_prob, output_dim)\n",
    "    loss = get_loss(output, targets)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    # clip gradients\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #writer = tf.summary.FileWriter(\"log\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "       \n",
    "#     # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "#     tvars = tf.trainable_variables()\n",
    "#     grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "#     train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "#     optimizer = train_op.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def word2seq(word):\n",
    "#     return np.array([vocab_to_int.get(c,0) for c in word])\n",
    "\n",
    "# def get_train_subset(model=w2v, seed_words=500, topn=7):\n",
    "#     top_words=model.wv.index2word[120:120+seed_words]\n",
    "#     top_words=np.append(np.array(top_words),np.array([np.array(model.most_similar_cosmul(w, topn=topn))[:,0] for w in top_words]))\n",
    "#     top_words=top_words.flatten()\n",
    "#     top_words=set(top_words)\n",
    "#     return top_words\n",
    "\n",
    "# input_list=list(get_train_subset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_padded_int_batch(input_batch, vocab_to_int=vocab_to_int):\n",
    "    max_len = max([len(word) for word in input_batch])\n",
    "    int_batch =  [[0] * (max_len - len(l)) + [vocab_to_int[w] for w in l] for l in input_batch]\n",
    "    return int_batch\n",
    "\n",
    "\n",
    "def get_batch(input_list=w2v.wv.index2word, batch_size=batch_size, vocab=vocab, vocab_to_int=vocab_to_int, model=w2v):\n",
    "    \"\"\"\n",
    "    Batch generator.\n",
    "    Input: train_set - list of words\n",
    "    Returns touple:\n",
    "    (pad_input_batch, pad_input_lengths, output_batch)\n",
    "    \"\"\"\n",
    "    for batch_i in range(0, len(input_list)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        input_batch = input_list[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_input_batch = np.array(get_padded_int_batch(input_batch, vocab_to_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_input_lengths = []\n",
    "        for line in pad_input_batch:\n",
    "            pad_input_lengths.append(len(line))\n",
    "    \n",
    "        # output batch\n",
    "        output_batch=np.array([w2v.wv.word_vec(w) for w in input_batch])\n",
    "\n",
    "        yield (pad_input_batch, pad_input_lengths, output_batch)\n",
    "\n",
    "\n",
    "# for (batch_i, (pad_input_batch, pad_input_lengths, output)) in enumerate(get_batch(w2v.wv.index2word[:1000], batch_size=50)):\n",
    "#     print (batch_i)\n",
    "#     pass\n",
    "\n",
    "train_size= 600000\n",
    "\n",
    "train_input = w2v.wv.index2word[:train_size]\n",
    "valid_input = w2v.wv.index2word[train_size:train_size+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./seq2vec\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs=100\n",
    "display_step=1\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        for batch_i, (pad_input_batch, pad_input_lengths, out_vec) in enumerate(get_batch(train_input)):\n",
    "\n",
    "            _, _, l = sess.run([initial_state, train_op, loss], {\n",
    "                input_: pad_input_batch,\n",
    "                targets: out_vec,\n",
    "                lr: learning_rate, \n",
    "                keep_prob: keep_probability,\n",
    "                input_sequence_length: pad_input_lengths,\n",
    "            })\n",
    "        if (epoch_i % display_step ==0):\n",
    "            (pad_input_batch, pad_input_lengths, out_vec) = next(get_batch(valid_input))\n",
    "            _, valid_loss = sess.run([initial_state, loss],{\n",
    "                input_: pad_input_batch,\n",
    "                targets: out_vec,\n",
    "                keep_prob: 1.0,\n",
    "                input_sequence_length: pad_input_lengths,\n",
    "            })\n",
    "            print(\"Epoch: {:3} | Loss: {:2.4}\\t validation loss: {:2.4}\".format(epoch_i, l, valid_loss))\n",
    "        # save model\n",
    "        loader.save(sess, save_dir)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learning_rate=0.000001\n",
    "\n",
    "num_epochs=7\n",
    "display_step=1\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        for batch_i, (pad_input_batch, pad_input_lengths, out_vec) in enumerate(get_batch(train_input)):\n",
    "\n",
    "            _, _, l = sess.run([initial_state, train_op, loss], {\n",
    "                input_: pad_input_batch,\n",
    "                targets: out_vec,\n",
    "                lr: learning_rate, \n",
    "                keep_prob: keep_probability,\n",
    "                input_sequence_length: pad_input_lengths,\n",
    "            })\n",
    "        if (epoch_i % display_step ==0):\n",
    "            (pad_input_batch, pad_input_lengths, out_vec) = next(get_batch(valid_input))\n",
    "            _, valid_loss = sess.run([initial_state, loss],{\n",
    "                input_: pad_input_batch,\n",
    "                targets: out_vec,\n",
    "                keep_prob: 1.0,\n",
    "                input_sequence_length: pad_input_lengths,\n",
    "            })\n",
    "            print(\"Epoch: {:3} | Loss: {:2.4}\\t validation loss: {:2.4}\".format(epoch_i, l, valid_loss))\n",
    "        # save model\n",
    "        loader.save(sess, save_dir)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load model and use a global session\n",
    "sess = tf.Session(graph=graph)\n",
    "loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "loader.restore(sess, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec(word, sess=sess):\n",
    "    pad_input_batch=get_padded_int_batch([word])\n",
    "    #print(len(pad_input_batch[0]))\n",
    "    _, outputs = sess.run([initial_state, output],{\n",
    "        input_: pad_input_batch,\n",
    "        keep_prob: 1.0,\n",
    "        input_sequence_length: [len(pad_input_batch[0])],\n",
    "    })\n",
    "    return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=train_input[200]\n",
    "\n",
    "word=\"ponieważ\"\n",
    "\n",
    "wordvec=get_word2vec(word)\n",
    "\n",
    "print(word)\n",
    "w2v.wv.similar_by_vector(wordvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word2vec(\"ula\")\n",
    "w2v.wv.index2word[990:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
