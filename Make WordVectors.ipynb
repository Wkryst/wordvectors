{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Original implementation, the generator version is in the next cell\n",
    "import nltk\n",
    "import os\n",
    "import codecs\n",
    "import argparse\n",
    "import numpy as np\n",
    "import gensim\n",
    "from itertools import chain\n",
    "import regex\n",
    "#from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "def get_min_count(sents, vocab_size):\n",
    "    '''\n",
    "    Args:\n",
    "      sents: A list of lists. E.g., [[\"I\", \"am\", \"a\", \"boy\", \".\"], [\"You\", \"are\", \"a\", \"girl\", \".\"]]\n",
    "     \n",
    "    Returns:\n",
    "      min_count: A uint. Should be set as the parameter value of word2vec `min_count`.   \n",
    "    '''\n",
    "    fdist = nltk.FreqDist(chain.from_iterable(sents))\n",
    "    min_count = fdist.most_common(vocab_size)[-1][1] # the count of the the top-kth word\n",
    "    \n",
    "    return min_count\n",
    "\n",
    "def make_wordvectors(txt_file, vector_size=300, window_size=5, vocab_size=50000, num_negative=5):\n",
    "    \n",
    "    print (\"Making sentences as list... {}\".format(txt_file))\n",
    "    word_count=0\n",
    "    sents = []\n",
    "    with codecs.open(txt_file, 'r', 'utf-8') as fin:\n",
    "        while 1:\n",
    "            line = fin.readline()\n",
    "            if not line: break\n",
    "             \n",
    "            words = line.split()\n",
    "            sents.append(words)\n",
    "            word_count+=len(words)\n",
    "    basename=regex.sub(\"-pages-articles-multistream\",\"\",txt_file[:-4])\n",
    "    basename=regex.sub(\"\\/\",\"/w2v-{}-{}-{}-{}-{}-\".format(word_count, vocab_size, vector_size, window_size, num_negative), basename)\n",
    "    #model_file=\"w2v-{}-{}-{}-{}-{}-{}bin\".format(word_count, vocab_size, vector_size, window_size, num_negative, basename)\n",
    "    model_file=\"{}.bin\".format(basename)\n",
    "    print (\"Making word vectors...\")\n",
    "    min_count = get_min_count(sents,vocab_size)\n",
    "    model = gensim.models.Word2Vec(sents, size=vector_size, min_count=min_count,\n",
    "                                   negative=num_negative, \n",
    "                                   window=window_size,\n",
    "                                   workers=10\n",
    "                                  )\n",
    "    \n",
    "    model.save(model_file)\n",
    "    \n",
    "    # Save to file\n",
    "    with codecs.open(\"{}.tsv\".format(basename), 'w', 'utf-8') as fout:\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            fout.write(u\"{}\\t{}\\t{}\\n\".format(str(i), word.encode('utf8').decode('utf8'),\n",
    "                                              np.array_str(model[word])\n",
    "                                              ))\n",
    "\n",
    "\n",
    "#make_wordvectors(\"data/plwikibooks-20170820-pages-articles-multistream.txt\")\n",
    "#make_wordvectors(\"data/OpenSubtitles2016.txt\")\n",
    "\n",
    "print (\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136078/136078 [00:01<00:00, 84708.70it/s]\n",
      "100%|██████████| 136078/136078 [00:07<00:00, 18209.55it/s]\n",
      "100%|██████████| 136078/136078 [00:07<00:00, 17157.83it/s]\n",
      "100%|██████████| 136078/136078 [00:07<00:00, 17087.01it/s]\n",
      "100%|██████████| 136078/136078 [00:07<00:00, 17403.33it/s]\n",
      "100%|██████████| 136078/136078 [00:07<00:00, 17076.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "import nltk\n",
    "import os\n",
    "import codecs\n",
    "import argparse\n",
    "import numpy as np\n",
    "import gensim\n",
    "import itertools\n",
    "import regex\n",
    "from six.moves import cPickle as pickle\n",
    "from os.path import isfile\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TextCorpus(object):\n",
    "    \"\"\"Iterate over sentences from the Text file.\"\"\"\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.line_count= self.get_line_count()\n",
    "\n",
    "    def __iter__(self):\n",
    "        with codecs.open(self.filename, 'r', 'utf-8') as fin:\n",
    "            for line in tqdm(fin, total=self.line_count):\n",
    "                words = line.split()\n",
    "                if not words:\n",
    "                    continue\n",
    "                yield words\n",
    "            \n",
    "    def get_line_count(self):\n",
    "        def blocks(files, size=65536):\n",
    "            while True:\n",
    "                b = files.read(size)\n",
    "                if not b: break\n",
    "                yield b\n",
    "\n",
    "        with codecs.open(self.filename, 'r', 'utf-8') as f:\n",
    "            return sum(bl.count(\"\\n\") for bl in blocks(f))\n",
    "\n",
    "# auxiliary finction for progress bar ;)\n",
    "def get_line_count(filename):\n",
    "    def blocks(files, size=65536):\n",
    "        while True:\n",
    "            b = files.read(size)\n",
    "            if not b: break\n",
    "            yield b\n",
    "\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        return sum(bl.count(\"\\n\") for bl in blocks(f))\n",
    "\n",
    "# Parsing 4.2GB in Wall time: 15min 9s\n",
    "def get_word_count(txt_file, overwrite=False):\n",
    "    basename=regex.sub(\"-pages-articles-multistream\",\"\",txt_file[:-4])\n",
    "    #basename=regex.sub(\"\\/\",\"/w2v-{}-{}-{}-{}-{}-\".format(), basename)\n",
    "    pickle_file=\"{}-wf.pickle\".format(basename)\n",
    "\n",
    "    if not isfile(pickle_file) or overwrite:\n",
    "        line_count=get_line_count(txt_file)\n",
    "        with codecs.open(txt_file, 'r', 'utf-8') as fin:\n",
    "            # memory efficient; count line by line\n",
    "            wordcounts = Counter()\n",
    "            for line in tqdm(fin, total=line_count):\n",
    "                wordcounts.update(line.split())\n",
    "#             wordcounts = Counter(itertools.chain.from_iterable([(line.split()) for line in fin]))\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(wordcounts, f, pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            wordcounts=pickle.load(f)\n",
    "    return wordcounts\n",
    "\n",
    "def text_file_generator(txt_file):\n",
    "    with codecs.open(txt_file, 'r', 'utf-8') as fin:\n",
    "        line_count=get_line_count(txt_file)\n",
    "        for line in tqdm(fin, total=line_count):\n",
    "            words = line.split()\n",
    "            if not words:\n",
    "                continue\n",
    "            yield words\n",
    "\n",
    "def make_wordvectors(txt_file, vector_size=300, window_size=5, vocab_size=50000, num_negative=5, skip_gram=1, save_tsv=False):\n",
    "    \n",
    "    wordcouns=get_word_count(txt_file)\n",
    "    min_count = wordcouns.most_common(vocab_size)[-1][1] # the count of the the top-kth word\n",
    "    word_count=sum(wordcouns.values())\n",
    "    \n",
    "    sentences = TextCorpus(txt_file)\n",
    "    \n",
    "    model = gensim.models.Word2Vec(sentences, size=vector_size, min_count=min_count,\n",
    "                                   negative=num_negative, \n",
    "                                   window=window_size,\n",
    "                                   sg=skip_gram,\n",
    "                                   workers=10\n",
    "                                  )\n",
    "    \n",
    "    basename=regex.sub(\"-pages-articles-multistream\",\"\",txt_file[:-4])\n",
    "    basename=regex.sub(\"\\/\",\"/w2v-{}-{}-{}-{}-{}-\".format(word_count, vocab_size, vector_size, window_size, num_negative), basename)\n",
    "    model_file=\"{}.bin\".format(basename)\n",
    "    model.save(model_file)\n",
    "    \n",
    "    if save_tsv:\n",
    "        # Save to tsv file\n",
    "        with codecs.open(\"{}.tsv\".format(basename), 'w', 'utf-8') as fout:\n",
    "            for i, word in enumerate(model.wv.index2word):\n",
    "                fout.write(u\"{}\\t{}\\t{}\\n\".format(str(i), word.encode('utf8').decode('utf8'),\n",
    "                                                  np.array_str(model[word])\n",
    "                                                  ))\n",
    "\n",
    "\n",
    "\n",
    "# fd=get_word_count(\"data/OpenSubtitles2016.txt\")\n",
    "# fd=get_word_count(\"data/plwikibooks-20170820-pages-articles-multistream.txt\", overwrite=True)\n",
    "# fd=get_word_count(\"data/plwiktionary-20170820-pages-articles-multistream.txt\")\n",
    "# fd=get_word_count(\"data/plwiki-20170820-pages-articles-multistream.txt\")\n",
    "# print(fd.most_common(1000000)[-1])\n",
    "\n",
    "make_wordvectors(\"data/plwiktionary-20170820-pages-articles-multistream.txt\")\n",
    "#make_wordvectors(\"data/OpenSubtitles2016.txt\")\n",
    "\n",
    "\n",
    "print (\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775384107"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143094531\n",
      "CPU times: user 9.08 s, sys: 716 ms, total: 9.8 s\n",
      "Wall time: 9.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (get_line_count(\"data/OpenSubtitles2016.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
