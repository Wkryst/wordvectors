{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sentences as list...\n"
     ]
    }
   ],
   "source": [
    "%run ./make_wordvectors.py --lcode=pl --vector_size=300 --window_size=5 --vocab_size=20000 --num_negative=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./make_wordvectors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./make_wordvectors.py\n",
    "# %load ./make_wordvectors.py\n",
    "import nltk\n",
    "import os\n",
    "import codecs\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "# arguments setting \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lcode', default='pl', help='ISO 639-1 code of target language. See `lcodes.txt`.')\n",
    "parser.add_argument('--vector_size', type=int, default=300, help='the size of a word vector')\n",
    "parser.add_argument('--window_size', type=int, default=5, help='the maximum distance between the current and predicted word within a sentence.')\n",
    "parser.add_argument('--vocab_size', type=int, default=20000, help='the maximum vocabulary size')\n",
    "parser.add_argument('--num_negative', type=int, default=5, help='the int for negative specifies how many “noise words” should be drawn')\n",
    "args = parser.parse_args()\n",
    "\n",
    "lcode = args.lcode\n",
    "vector_size = args.vector_size\n",
    "window_size = args.window_size\n",
    "vocab_size = args.vocab_size\n",
    "num_negative = args.num_negative\n",
    "\n",
    "def get_min_count(sents):\n",
    "    '''\n",
    "    Args:\n",
    "      sents: A list of lists. E.g., [[\"I\", \"am\", \"a\", \"boy\", \".\"], [\"You\", \"are\", \"a\", \"girl\", \".\"]]\n",
    "     \n",
    "    Returns:\n",
    "      min_count: A uint. Should be set as the parameter value of word2vec `min_count`.   \n",
    "    '''\n",
    "    global vocab_size\n",
    "    from itertools import chain\n",
    "     \n",
    "    fdist = nltk.FreqDist(chain.from_iterable(sents))\n",
    "    min_count = fdist.most_common(vocab_size)[-1][1] # the count of the the top-kth word\n",
    "    \n",
    "    return min_count\n",
    "\n",
    "def make_wordvectors():\n",
    "    global lcode\n",
    "    import gensim # In case you have difficulties installing gensim, you need to consider installing conda.\n",
    "#    from six.moves import cPickle as pickle\n",
    "     \n",
    "    print (\"Making sentences as list...\")\n",
    "    sents = []\n",
    "    with codecs.open('data/{}.txt'.format(lcode), 'r', 'utf-8') as fin:\n",
    "        while 1:\n",
    "            line = fin.readline()\n",
    "            if not line: break\n",
    "             \n",
    "            words = line.split()\n",
    "            sents.append(words)\n",
    "\n",
    "    print (\"Making word vectors...\")\n",
    "    min_count = get_min_count(sents)\n",
    "    model = gensim.models.Word2Vec(sents, size=vector_size, min_count=min_count,\n",
    "                                   negative=num_negative, \n",
    "                                   window=window_size)\n",
    "    \n",
    "    model.save('data/{}.bin'.format(lcode))\n",
    "    \n",
    "    # Save to file\n",
    "    with codecs.open('data/{}.tsv'.format(lcode), 'w', 'utf-8') as fout:\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            fout.write(u\"{}\\t{}\\t{}\\n\".format(str(i), word.encode('utf8').decode('utf8'),\n",
    "                                              np.array_str(model[word])\n",
    "                                              ))\n",
    "if __name__ == \"__main__\":\n",
    "    make_wordvectors()\n",
    "    \n",
    "    print (\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
