{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Original implementation, the generator version is in the next cell\n",
    "import nltk\n",
    "import os\n",
    "import codecs\n",
    "import argparse\n",
    "import numpy as np\n",
    "import gensim\n",
    "from itertools import chain\n",
    "import regex\n",
    "#from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "def get_min_count(sents, vocab_size):\n",
    "    '''\n",
    "    Args:\n",
    "      sents: A list of lists. E.g., [[\"I\", \"am\", \"a\", \"boy\", \".\"], [\"You\", \"are\", \"a\", \"girl\", \".\"]]\n",
    "     \n",
    "    Returns:\n",
    "      min_count: A uint. Should be set as the parameter value of word2vec `min_count`.   \n",
    "    '''\n",
    "    fdist = nltk.FreqDist(chain.from_iterable(sents))\n",
    "    min_count = fdist.most_common(vocab_size)[-1][1] # the count of the the top-kth word\n",
    "    \n",
    "    return min_count\n",
    "\n",
    "def make_wordvectors(txt_file, vector_size=300, window_size=5, vocab_size=50000, num_negative=5):\n",
    "    \n",
    "    print (\"Making sentences as list... {}\".format(txt_file))\n",
    "    word_count=0\n",
    "    sents = []\n",
    "    with codecs.open(txt_file, 'r', 'utf-8') as fin:\n",
    "        while 1:\n",
    "            line = fin.readline()\n",
    "            if not line: break\n",
    "             \n",
    "            words = line.split()\n",
    "            sents.append(words)\n",
    "            word_count+=len(words)\n",
    "    basename=regex.sub(\"-pages-articles-multistream\",\"\",txt_file[:-4])\n",
    "    basename=regex.sub(\"\\/\",\"/w2v-{}-{}-{}-{}-{}-\".format(word_count, vocab_size, vector_size, window_size, num_negative), basename)\n",
    "    #model_file=\"w2v-{}-{}-{}-{}-{}-{}bin\".format(word_count, vocab_size, vector_size, window_size, num_negative, basename)\n",
    "    model_file=\"{}.bin\".format(basename)\n",
    "    print (\"Making word vectors...\")\n",
    "    min_count = get_min_count(sents,vocab_size)\n",
    "    model = gensim.models.Word2Vec(sents, size=vector_size, min_count=min_count,\n",
    "                                   negative=num_negative, \n",
    "                                   window=window_size,\n",
    "                                   workers=10\n",
    "                                  )\n",
    "    \n",
    "    model.save(model_file)\n",
    "    \n",
    "    # Save to file\n",
    "    with codecs.open(\"{}.tsv\".format(basename), 'w', 'utf-8') as fout:\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            fout.write(u\"{}\\t{}\\t{}\\n\".format(str(i), word.encode('utf8').decode('utf8'),\n",
    "                                              np.array_str(model[word])\n",
    "                                              ))\n",
    "\n",
    "\n",
    "#make_wordvectors(\"data/plwikibooks-20170820-pages-articles-multistream.txt\")\n",
    "#make_wordvectors(\"data/OpenSubtitles2016.txt\")\n",
    "\n",
    "print (\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import codecs\n",
    "import argparse\n",
    "import numpy as np\n",
    "import gensim\n",
    "import itertools\n",
    "import regex\n",
    "from six.moves import cPickle as pickle\n",
    "from os.path import isfile\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repl_dict={    \n",
    "    '.': '||period||',\n",
    "    ',': '||comma||',\n",
    "    '\"': '||quotation_mark||',\n",
    "    ';': '||semicolon||',\n",
    "    '!': '||exclamation_mark||',\n",
    "    '?': '||question_mark||',\n",
    "    '(': '||left_parenthesis||',\n",
    "    ')': '||right_parenthesis||',\n",
    "#     '--': '||dash||',\n",
    "    '?': '||question_mark||',\n",
    "#     '\\n': '||return||'\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Perform a simple multiple replace.\n",
    "    repl_dict has to be a dictionary i replace patterns.\n",
    "    The key cannot contain patterns.\n",
    "    \"\"\"\n",
    "    # build regexp\n",
    "    reg_exp = regex.compile(\"|\".join(map(regex.escape, repl_dict.keys())))\n",
    "\n",
    "    # replace :)\n",
    "    return reg_exp.sub(lambda match: repl_dict[match.group(0)], text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143094531/143094531 [14:06<00:00, 169061.93it/s]\n",
      "100%|██████████| 143094531/143094531 [33:51<00:00, 70448.73it/s] \n",
      "100%|██████████| 143094531/143094531 [33:04<00:00, 72120.17it/s] \n",
      "100%|██████████| 143094531/143094531 [33:03<00:00, 72157.37it/s] \n",
      "100%|██████████| 143094531/143094531 [33:09<00:00, 71924.27it/s] \n",
      "100%|██████████| 143094531/143094531 [33:14<00:00, 71744.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "\n",
    "class TextCorpus(object):\n",
    "    \"\"\"\n",
    "    Iterate over sentences from the Text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, line_count=None):\n",
    "        self.filename = filename\n",
    "        self.line_count= line_count\n",
    "\n",
    "    def __iter__(self):\n",
    "        with codecs.open(self.filename, 'r', 'utf-8') as fin:\n",
    "            for line in tqdm(fin, total=self.line_count):\n",
    "                words = line.split()\n",
    "                if not words:\n",
    "                    continue\n",
    "                yield words\n",
    "            \n",
    "\n",
    "def get_line_count(filename):\n",
    "    \"\"\"\n",
    "    Calculate numer of lines in the file\n",
    "    \"\"\"\n",
    "    def blocks(files, size=65536):\n",
    "        while True:\n",
    "            b = files.read(size)\n",
    "            if not b: break\n",
    "            yield b\n",
    "\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        return sum(bl.count(\"\\n\") for bl in blocks(f))\n",
    "\n",
    "# Parsing 4.2GB in Wall time: 15min 9s\n",
    "def get_word_count(txt_file, overwrite=False):\n",
    "    \"\"\"\n",
    "    Calculate number of words in text file using Counter (rather than nltk.FreqDist).\n",
    "    It can take several minutes for large files so it picles the results for faster retrieval.\n",
    "    \"\"\"\n",
    "    basename=regex.sub(\"-pages-articles-multistream\",\"\",txt_file[:-4])\n",
    "    pickle_file=\"{}-wf.pickle\".format(basename)\n",
    "\n",
    "    if not isfile(pickle_file) or overwrite:\n",
    "        line_count=get_line_count(txt_file)\n",
    "        with codecs.open(txt_file, 'r', 'utf-8') as fin:\n",
    "            # memory efficient; count line by line\n",
    "            wordcounts = Counter()\n",
    "            for line in tqdm(fin, total=line_count):\n",
    "                wordcounts.update(line.split())\n",
    "#             wordcounts = Counter(itertools.chain.from_iterable([(line.split()) for line in fin]))\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(wordcounts, f, pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            wordcounts=pickle.load(f)\n",
    "    return wordcounts\n",
    "\n",
    "\n",
    "def get_text_file(txt_file, line_count=None):\n",
    "    \"\"\"\n",
    "    Read text file up into memory and return it as list of lines.\n",
    "    Each line is a list of words.\n",
    "    The python implementation for list of lists is super inefficient. \n",
    "    For large text files it can take over 10 times more memory than the source file.\n",
    "    In such case use TextCorpus() class that returns iterator that you can plug in to Word2Vec\n",
    "    \"\"\"\n",
    "    fflush_iterations = 500000\n",
    "    #with codecs.open(txt_file, 'r', 'utf-8') as fin:\n",
    "    with open(txt_file, \"r\", encoding='utf_8') as fin:\n",
    "        sents = [str.split(line) for line in tqdm(fin, total=line_count)]\n",
    "    return sents\n",
    "\n",
    "def make_wordvectors(txt_file, vector_size=300, window_size=5, vocab_size=50000, num_negative=5,\n",
    "                     skip_gram=1, save_tsv=False, workers=4):\n",
    "    \"\"\"\n",
    "    Build Word2Vec from provided corpus.\n",
    "    \n",
    "    \"\"\"\n",
    "    # read word counters\n",
    "    wordcouns=get_word_count(txt_file)\n",
    "    # determine the lowest frequency to match vocabulary size requirement\n",
    "    min_count = wordcouns.most_common(vocab_size)[-1][1] # the count of the the top-kth word\n",
    "    \n",
    "    # need it for progress bar only\n",
    "    line_count=get_line_count(txt_file)\n",
    "    \n",
    "    # Use generator for larger files\n",
    "    if line_count > 50000000:\n",
    "        sentences = TextCorpus(txt_file, line_count)\n",
    "    else:\n",
    "        # for smaller files read the file up to memory\n",
    "        sentences = get_text_file(txt_file,line_count)\n",
    "    \n",
    "    print (\"Building word2vec\")\n",
    "    model = gensim.models.Word2Vec(sentences, size=vector_size, min_count=min_count,\n",
    "                                   negative=num_negative, \n",
    "                                   window=window_size,\n",
    "                                   sg=skip_gram,\n",
    "                                   workers=workers\n",
    "                                  )\n",
    "    # construct filename\n",
    "    word_count=sum(wordcouns.values())\n",
    "    basename=regex.sub(\"-pages-articles-multistream\",\"\",txt_file[:-4])\n",
    "    basename=regex.sub(\"\\/\",\"/w2v-{}-{}-{}-{}-{}-\".format(word_count, vocab_size, vector_size, window_size, num_negative), basename)\n",
    "    model_file=\"{}.bin\".format(basename)\n",
    "    model.save(model_file)\n",
    "    \n",
    "    if save_tsv:\n",
    "        # Save to tsv file\n",
    "        with codecs.open(\"{}.tsv\".format(basename), 'w', 'utf-8') as fout:\n",
    "            for i, word in enumerate(model.wv.index2word):\n",
    "                fout.write(u\"{}\\t{}\\t{}\\n\".format(str(i), word.encode('utf8').decode('utf8'),\n",
    "                                                  np.array_str(model[word])\n",
    "                                                  ))\n",
    "\n",
    "\n",
    "\n",
    "# fd=get_word_count(\"data/OpenSubtitles2016.txt\")\n",
    "# fd=get_word_count(\"data/plwikibooks-20170820-pages-articles-multistream.txt\", overwrite=True)\n",
    "# fd=get_word_count(\"data/plwiktionary-20170820-pages-articles-multistream.txt\")\n",
    "# fd=get_word_count(\"data/plwiki-20170820-pages-articles-multistream.txt\")\n",
    "# print(fd.most_common(1000000)[-1])\n",
    "\n",
    "# make_wordvectors(\"data/plwiktionary-20170820-pages-articles-multistream.txt\")\n",
    "make_wordvectors(\"data/OpenSubtitles2016.txt\", vocab_size=1000000, workers=10)\n",
    "\n",
    "\n",
    "print (\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
